实验 E1–E5：运行方法与输出

本节逐一介绍五组实验 (E1–E5) 的设计目的、运行方法和预期输出。每个实验通过不同配置侧重点验证算法的各个方面。请按照说明运行对应实验，并参考输出结果进行分析。

实验概览

E1 – 随机移除策略基准：使用随机移除算子的HEA，作为基准线比较解质量和运行效率。

E2 – Shaw移除策略基准：使用经典Shaw启发式移除，提高解质量，评估其优于随机的幅度。

E3 – DRL移除策略效果：使用预训练的DRL智能移除策略，验证其相对Shaw的性能提升，并观察运行开销变化。

E4 – 算法可扩展性测试：在更大规模实例上运行HEA算法，比较Shaw和DRL策略的解质量及效率，评估算法扩展到复杂场景的能力。

E5 – DRL策略训练过程：不比较解质量，而是展示DRL代理的训练过程，包括训练耗时和策略收敛曲线，支持此前实验中DRL模型的有效性。

以下是各实验的详细说明：

E1：随机移除策略 (Random Removal Baseline)

目的：以最简单的随机移除策略为局部搜索算子，运行HEA算法，获取基准性能数据。此实验将展示HEA在无智能启发的情况下能达到的解质量和耗时，为后续比较提供参考。

设置：

移除策略：Random (随机移除)

问题规模：中等规模实例，如 50 个任务。使用统一的随机生成实例。

配置文件：configs/E1.yaml 定义上述策略和参数。确认其中 removal_strategy: "random"。

运行命令：在项目根目录执行:

python main.py --config configs/E1.yaml


程序将根据配置生成实例、运行HEA算法并输出日志。

输出：

日志文件：results/E1/E1.log（假定文件名，以配置为准）。其中包含每代的最佳适应度值、时间统计等。关注日志末尾记录的最终目标值和总耗时。

图表：可能生成收敛曲线图 results/E1/convergence.png，显示代数 vs. 目标值。如未自动生成，可使用日志数据自行绘制。

表格：本实验主要作为基准，可将其最终结果（例如最佳目标值）整理成表，用于与E2/E3比较。

预期结果：随机移除策略下，解质量相对较差，算法可能需要较多代数收敛且容易停留局部最优。运行时间应是几种策略中最少的（因为移除和修复操作简单）。此实验的数据将为后续评估“引入更智能策略提升了多少解质量”提供起点。

E2：Shaw 移除策略

目的：评估 Shaw 相似性启发式移除对算法性能的提升。比较 E1 和 E2，可看出利用问题结构信息（相似任务一起移除）带来的解改进以及运行代价变化。

设置：

移除策略：Shaw (相似性启发式)

问题规模：与 E1 相同，以保证结果可比（例如也是50任务的相同实例或相同规模随机实例，具体由配置中的 seed 控制）。

配置文件：configs/E2.yaml，其中 removal_strategy: "shaw"。

运行命令：

python main.py --config configs/E2.yaml


运行前确保如有需要，已根据 E1 输出的实例固定种子，以便两次实验在相同数据上对比。（如配置相同 random_seed 则自动保证数据一致）。

输出：

日志文件：results/E2/E2.log。日志格式与E1类似，记录每代最优值和时间。

关键观察：最后的最佳解目标值预期优于 E1（随机策略），同时每代改进幅度可能更大。记录最终目标值以供比对。

图表：可能生成 results/E2/convergence.png。将其与E1的曲线对比，可直观看出Shaw策略收敛更快、最终值更优。

性能数据：时间日志中，Shaw策略的 Remove 耗时可能略高于随机。但总体迭代次数可能减少，因更快逼近最优。从总耗时看，Shaw可能略快或相当（取决于改进效率弥补的算子开销）。

预期结果：Shaw移除将显著提高解质量，例如降低总成本或延迟。一些实例中可能用更少代数达到随机策略无法达到的目标值。此验证了利用领域知识（相似性）能有效增强局部搜索。但需注意Shaw策略在大规模情况下耗时增长情况，为后续E4埋下伏笔。

E3：DRL 移除策略

目的：测试预训练的DRL智能移除策略在求解中的效果。比较其与Shaw策略的表现，检验学习策略是否进一步提升了解质量，同时评估其运行开销。

设置：

移除策略：DRL (智能策略)，使用事先训练好的模型。

问题规模：与 E2 相同（例如50任务），以公平比较。理想情况下使用相同测试实例或同分布随机实例。

配置文件：configs/E3.yaml。应包括 removal_strategy: "drl"，以及 model_path: "models/drl_model.pth" 指向正确的模型文件。如果模型文件名或路径不同，请相应修改。还可设置 use_gpu 来决定是否使用GPU加速推理（需确保环境支持）。

运行命令：

python main.py --config configs/E3.yaml


在运行前，请确认 models/ 目录下存在有效的 DRL 模型文件。如果没有，请先执行 E5 训练模型。

输出：

日志文件：results/E3/E3.log。重点关注：

最终最佳解目标值：预期比 E2 (Shaw) 更优。如果DRL模型有效，解质量应提升（哪怕幅度可能不大，取决于问题复杂度）。

收敛代数：DRL策略可能更快收敛至高质量解，或者达到更优值。观察日志中何时找到最优解，与E2对比。

时间开销：日志末的时间分布会显示 DRL_infer 时间。例如 [Time] Total: ... DRL_infer=XYZ. 对比 E2 没有该项。若DRL_infer占总时间的比例较小，则说明引入智能策略的效率代价不高。

图表：

results/E3/convergence.png：DRL策略的收敛曲线。如果绘制E2与E3同图，将看到E3曲线更快下降或达到更低的目标值。

可选的对比图：项目可能提供脚本将 E1/E2/E3 的最终结果绘制成柱状图或表格，直观显示解质量差异。

输出解方案：如果 save_solution: true，将在 results/E3/ 下生成最终解的详细方案文件（例如 best_solution.json）。可与E2的方案对比，看看DRL选择的移除是否产生明显不同的路线/调度方案。

预期结果：DRL移除策略应实现最佳的解质量，优于Shaw启发式。这验证了深度强化学习成功学到了有效的策略规则。例如，总成本可能降低若干百分比，或延迟减少显著。同时，DRL的运行耗时略有增加但不至于失控——例如总时间只比Shaw多20%，而获得了更好的解。这个权衡对于实际应用很重要。若结果未达预期（DRL未胜出），需检查训练是否充分或问题规模是否超出模型泛化范围。

E4：可扩展性与效率测试

目的：在更大规模或不同分布的实例上测试算法，考察Shaw和DRL策略在扩展情况下的表现差异，以及算法整体可扩展性。通过该实验可以了解，当问题变难时，各策略的质量提升和时间开销如何变化。

设置：

实例规模：显著大于 E2/E3。例如若之前为50任务，这里测试100或200任务实例。也可测试不同数据分布（如之前均匀分布，这里测试簇状分布）。

策略方案：可以分别运行 Shaw 和 DRL 两种策略在大规模实例下的效果。为此，我们提供两个配置:

configs/E4_shaw.yaml：Shaw策略，大规模实例参数。

configs/E4_drl.yaml：DRL策略，大规模实例参数。
（或者采用单一配置，以DRL策略运行，并与E2结果对比规模效应。更推荐分别运行，以直接对比同规模下两策略表现。）

运行命令：

python main.py --config configs/E4_shaw.yaml
python main.py --config configs/E4_drl.yaml


由于大规模实例计算量较大，可考虑适当减少代数或种群规模以控制运行时间，在配置中做相应调整，同时保证两策略配置一致可比。

输出：

日志文件：

results/E4_shaw/E4_shaw.log

results/E4_drl/E4_drl.log
查看每个的最终结果和时间。其中关键是比较：

最终目标值：DRL策略是否依然优于Shaw？随着规模增大，差距可能拉大或缩小，需要通过结果判断。

收敛行为：两者的收敛曲线在大规模下可能都较慢，但是否出现DRL更快收敛的趋势。

运行时间：总耗时方面，大规模下两者可能都有明显增长。关注DRL_infer耗时占比在大实例下是否显著上升。如果DRL每次决策为O(n)操作，那么n翻倍理论上推理时间翻倍，但Shaw可能是O(n log n)亦会增长。日志提供的数据可支持这个复杂度推断。

图表：

对比解质量图：若运行了两种策略，可将它们的最终目标值绘制成柱状图，标题注明任务规模。例如“100任务实例下不同策略结果”。

对比时间图：将Shaw vs DRL总耗时、细分类别耗时绘制对比。例如堆叠柱状图展示不同部分耗时占比，观察DRL推理部分在总耗时中的比例变化。

收敛曲线：各自绘制在大规模下的收敛曲线图，如 convergence_shaw_100.png vs convergence_drl_100.png。

其他输出：保存的最终解方案，可用于验证解可行性及差异。例如车辆数或路线长度对比，判断两策略何处改进。

预期结果：E4将展示算法的扩展能力：

在大规模实例中，算法仍能找到可行解，但需要更多时间。Shaw和DRL都可能遇到更长收敛过程。

DRL策略有望继续取得比Shaw更优的解，但优势幅度可能受规模影响。也许在100任务时DRL优于Shaw的百分比提升与50任务时相当，表明其泛化良好；或者略有下降，说明模型需针对大规模调整。

运行效率上，DRL策略相对Shaw在大规模下的时间成本需要关注。如果DRL推理耗时增长平缓（例如占总时间从15%升至20%），则可以接受；若增长过多，可能需要优化模型或采取分批决策策略。

此实验有助于证明HEA-DRL方法的实用性——在更接近实际规模的问题上，智能策略依然有效且可承受运行时间。这将为框架的应用前景提供支撑。

E5：DRL 策略训练实验

目的：展示DRL移除策略的训练过程和收敛效果，为之前实验中使用的DRL模型提供依据。通过训练日志和曲线，我们可以看到智能体的学习曲线，以及选择不同训练参数对最终策略性能的影响。

设置：

训练环境：使用相对较小规模的问题实例进行训练（如20-50任务），以保证单次模拟速度和训练收敛速度。在训练过程中，代理会反复在不同随机实例上学习。

训练配置：configs/train_DRL.yaml 或类似文件，包含训练相关设置：

train_episodes 或 train_iterations：训练轮数，如10000次迭代。

learning_rate, batch_size 等：学习率、批大小等RL算法超参数。

reward_function 设置：奖励计算方式（如使用成本改进）。

validation_interval：间隔评估频率等。

（训练也可能复用 E5.yaml 区分 mode，如 mode: train）

运行命令：

python train_drl.py --config configs/train_DRL.yaml


如果没有独立脚本，也可能通过 main.py 加参数进入训练模式，例如:

python main.py --config configs/E5.yaml --train


（具体依据实现而定，以相应文档/注释为准。）

输出：

训练日志：results/E5/train.log，记录训练过程中每个迭代或每隔一定迭代的指标：

平均每回合奖励 (reward)。

当前策略评估性能（例如在验证集上的平均解质量）。

训练耗时进度，如每100回合耗时。

如有使用LOSS，价值网络损失等指标，也可记录。

模型文件：训练完成后，会自动保存最终模型到 models/drl_model.pth（或带时间戳/实验编号的文件）。该模型将用于E3、E4实验。如果训练提前有checkpoint，可能也保存中间模型用于测试。

图表：

学习曲线：results/E5/training_curve.png，绘制累计奖励或解质量随训练迭代的曲线。期望看到曲线逐步上升趋于稳定，表示策略在提升。

验证曲线：如有定期验证，可绘制验证集平均成本随训练的变化，验证曲线通常与训练曲线趋势一致，且用于判断过拟合。

损失曲线：（如果记录损失）可显示训练过程中策略/价值网络损失的下降。

预期结果：

训练初期，随机策略奖励偏低，波动较大。随着训练进行，智能体逐渐学会移除策略模式，平均奖励会提高并收敛。

最终训练得到的模型，其策略在验证集上应该优于随机甚至接近Shaw效果。如果结果表明训练充分（奖励饱和且验证表现好），则E3实验中的DRL效果应当凸显。

训练总耗时会记录在日志，如 "Training completed in X seconds". 这也提供了DRL开发的时间成本概念。

注意事项：

训练由于涉及随机性，建议固定随机种子以便结果复现和曲线平滑。如结果波动较大，可增加训练时间或调整超参数。

如果需要再次训练调整参数，请备份原模型，以免覆盖影响E3/E4结果对比。可以使用不同模型文件名区分不同训练版本。

通过E5实验，我们直观了解了DRL策略的来源和可靠性。这一过程对理解DRL模块至关重要，亦证明本文档所述框架完整闭环：从训练到推理，对比传统启发式，形成一个循证的研究。

实验结果总结与分析

完成以上实验后，用户应掌握如下结论要点：

移除策略对HEA算法效果有重大影响：随机 < Shaw < DRL 逐级提升解质量，同时算法效率略有变化但总体可接受。

HEA-DRL 框架在中等规模问题上取得了优秀结果，在更大规模问题上也具备较好的扩展性（尽管需要更多计算时间）。

DRL策略的有效性来源于充分的训练，其学习曲线和泛化能力通过E5和E4得到验证。

通过各实验的日志和图表，可为论文/报告提供有力的数据支撑。例如：表格比较三种策略最终结果，图表展示收敛和耗时差异，证明引入智能策略的优势。

请结合 Results.md 中的目录和数据说明，对实验输出进行整理分析。如有需要，可进一步尝试额外实验（例如不同移除数量、不同问题分布），本框架支持快速配置运行。这些研究将为本地实验结论提供全面支持。