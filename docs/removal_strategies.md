移除策略接口

在本框架的局部搜索过程中，“移除策略”承担着破坏算子的角色。移除策略接口定义了如何从当前解中选择一组任务移除，为后续重插入优化创造空间。本节介绍移除策略接口的设计，以及三种具体实现：随机移除、Shaw 移除和DRL 移除。通过统一接口设计，开发者可以方便地切换策略或添加新的移除算子。

接口设计与调用

移除策略接口封装为一个抽象函数，例如：

remove_tasks(solution, num_remove) -> list_removed_tasks


solution：当前解对象，包含任务的排列或分配信息。

num_remove：需要移除的任务数量。

返回值：应移除的任务列表（或标识）。

HEA 核心在局部搜索时会调用此接口获取待移除任务的集合。不同策略实现此接口时，使用各自算法选出任务，但对HEA核心来说过程透明。接口设计保证了：

统一性：不论采用哪种策略，核心调用方式相同，这使得我们可以在配置中以字符串或枚举形式指定策略类型，然后在运行时由工厂模式或if判断实例化对应策略对象并调用。

可调参数：num_remove 可以根据需要设定为固定值或由策略内部决定/调整。一些策略可根据解规模动态调整移除量，但核心会提供一个建议值或上限。

在实现中，框架提供了一个基类 (如 RemovalStrategy)，不同策略继承并实现 remove_tasks 方法。这使得添加新策略时，只需新增子类并注册，无需改动核心流程代码。

随机移除 (Random Removal)

随机移除策略是一种基准方法，不利用任何启发信息，纯粹随机地选择任务移除。其特点和实现：

逻辑：在当前解的任务集合中，随机抽取 num_remove 个任务。可通过随机洗牌或直接使用随机索引选取。

特点：实现简单，速度极快（O(n) 内选出要移除的任务）。作为对比基准，它缺乏针对性，有可能移除不恰当的任务导致解质量变差。但也因为随机，提供了较大的扰动多样性。

可配置：可以设定随机种子确保结果复现。num_remove 通常由配置给定固定值或按比例计算（例如移除总任务数的一定百分比）。

接口实现：随机策略通常不需要太多额外参数。在实现的 remove_tasks 中，调用伪随机数生成器选取任务ID列表返回即可。

Shaw 移除 (相似性启发式)

Shaw 移除策略是一种经典启发式，由 Shaw (1998) 提出
link.springer.com
。其核心思想是移除一组彼此相似的任务，因为这些任务重新安插后更可能打破原有次优结构从而改进解。
link.springer.com
具体特点：

相似性度量：首先随机选择一个种子任务，然后根据与种子任务的“距离”选择其它任务。相似性可以根据多种因素衡量，例如：

两任务地理位置的距离（越近越相似）。

时间窗或服务时间的接近程度（时间上相关的任务）。

任务的需求或属性（需求量相近，或者在同一路径中）。

综合上述因素计算一个相似性分值，分值高表示任务彼此相似度高。

移除选择：选定种子任务后，迭代地选择下一个最相似的任务加入移除集，直到达到 num_remove 数量。这样确保被移除的一组任务在原解中彼此关联紧密。

期望效果：由于这些任务关联性强，原解往往将它们安排得也较紧密。一次性移除后，修复过程可能重新排列这些相关任务，打开更优安排的可能性。如果选取的任务毫无关联（如随机情况），重插时大概率放回原位，局部搜索效果有限
link.springer.com
。

复杂度：计算两两任务相似度会增加一些开销，但一般通过预计算或限制候选集合可以接受。在实例规模较大时，Shaw移除相对随机会稍慢一点，但往往带来更好的解优化效果。

接口实现：Shaw策略实现中，需要定义一个相似度计算函数，然后以种子为起点逐步选满任务。实现时会用到问题的领域信息（如坐标距离公式、时间差计算等），因此比随机策略代码更复杂，但使用接口形式对核心透明。

DRL 移除 (智能策略)

DRL移除策略由训练好的深度强化学习代理实现（详见 DRL_Module.md）。作为移除策略的一种，DRL 策略通过学习得到的策略网络选择任务：

逻辑：DRL策略不使用人工设定的规则，而是通过智能体模型对当前解做出决策。具体实现上，DRL代理会对当前解进行多步决策，逐一选择要移除的任务，直到达到数量 num_remove。每一步选择由神经网络根据状态输出，并可能带有随机探索成分（在训练时），但推理阶段一般选择概率最高的动作。

特点：DRL策略力图综合考虑多种因素（距离、时间、负载等），其决策准则是直接优化最终解质量（通过训练奖励设计体现）。因此，它可能辨识出某些复杂模式下应该移除哪些任务，这是人工启发式难以穷举的。理想情况下，DRL策略应当表现出比Shaw更好的移除效果，在相同修复策略下产生更优解。

接口实现：DRL策略的 remove_tasks 方法内部会调用预先加载的DRL模型。模型对输入的解状态进行推理，输出建议移除的任务列表。实现需确保调用模型高效，并处理模型输出格式（例如概率转为具体任务选择）。如果模型未加载或预测异常，系统需要给出相应提示或回退到默认策略。

局限：DRL策略的表现取决于训练质量。若训练数据不足或策略收敛不佳，可能输出不合理的移除选择。但在良好训练下，其效果通常优于固定启发式。运行时，DRL决策比纯启发式稍耗时，但本项目有相应时间记录以权衡此成本。

切换与比较策略

由于采用统一接口，切换移除策略只需在配置中修改参数。例如：

removal_strategy: "random"   # 改为 "shaw" 或 "drl" 可切换策略
remove_count: 10            # 每次移除任务数量


通过以上配置，核心算法将实例化对应的策略类 (RandomRemoval, ShawRemoval, DRLRemoval) 来使用。开发者可以方便地针对同一问题切换不同策略运行实验，比较结果。

典型地，我们预期：

随机移除 (E1实验) 作为对照组，解质量最低但运行最快。

Shaw移除 (E2实验) 能提升解质量，相对牺牲一点运行时间。

DRL移除 (E3实验) 若训练良好，将取得最好的解质量，同时运行时间可能略高于Shaw。具体差异可在实验结果中通过日志分析和图表比较得出。

扩展新的移除策略

本框架支持开发者扩展自定义的移除策略：

新建一个类，例如 MyRemovalStrategy，继承基类并实现 remove_tasks 接口。在该方法中编写自定义的任务选择逻辑。

将新策略注册到系统的策略工厂或配置解析中。例如在读取配置时，解析 removal_strategy 字段：如果值匹配 "my_strategy"，则实例化 MyRemovalStrategy。

在配置文件中使用新策略名称，即可无缝替换移除算子。

通过这种方式，可以轻松尝试各种移除启发式（如基于任务优先级的移除、基于路径贡献度的移除等）。框架的其他部分（HEA核心、修复策略等）无需修改，即可与新移除策略协同工作。这体现了系统设计的灵活性和可扩展性。