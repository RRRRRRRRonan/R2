DRL 模块与插拔接口

本节介绍深度强化学习 (Deep Reinforcement Learning, DRL) 模块在本系统中的设计和作用。DRL 模块通过学习智能“移除”策略，提高算法对复杂问题结构的自适应能力。我们阐述DRL代理的状态动作设计、训练过程以及与主框架的接口集成。

模块概述及作用

DRL 模块实现了一个智能移除策略代理，可视为一种可选的移除算子。当配置使用“DRL移除”策略时，HEA 核心将在局部搜索阶段调用该模块，由智能代理决定应从当前解中移除哪些任务。其目标是学会比人工规则（随机或Shaw启发式）更优的选择策略，从而破坏掉那些妨碍解改进的任务组合，为修复创造更优机会。

作用：通过训练，DRL代理能够识别解的关键特征，并针对不同解结构做出较优的移除决策，提高算法最终求解质量。特别是在复杂约束场景下，人工设计的启发式可能不完美，而DRL有望自动学习出接近最优的移除方案。

状态、动作与奖励设计

为了应用DRL解决移除决策，我们需要定义强化学习的基本要素：

环境状态 (State)：应能表示当前解的特征。状态通常包括当前解中各任务/客户的相关属性（例如任务位置、时间窗、需求、当前路线负载等）以及整体解的评价指标。由于直接输入整个解较复杂，我们采用特征提取方法，将每个任务提取若干维度特征，组合成当前解的表示向量。DRL代理基于这些特征感知当前解局面。

动作 (Action)：指选择要移除的一批任务。DRL代理需要从当前解的任务集合中选择若干任务予以删除。可能的方式有两种：

单步选择：一次动作选择一个任务，移除后环境状态更新，再由代理继续选择下一个，直到选满预定数量k个任务。

一次性选择：代理直接输出一个包含k个任务的集合作为动作。

本框架更倾向于逐步选择的建模，代理每次从状态出发选择下一个要移除的任务，连续决策k次构成移除集。这种方法将多任务选择问题拆解为序列决策，降低动作空间难度。

奖励 (Reward)：设计体现移除决策的好坏。通常在完成一次完整的移除-重插后，根据解质量的变化来定义奖励。例如：$Reward = -(新解的目标成本 - 原解的目标成本)$，这样如果移除/重插导致了解改进（成本下降），奖励为正；若解变差，奖励为负。通过强化学习，代理将学习选择能最大化长期奖励的移除动作序列。为稳定训练，可以在每次移除一步后给予中间奖励或仅在最终插入完成后给累计奖励。

此外，需要考虑约束可行性：代理的动作不能使解不可修复或违反约束。本环境通过约束在状态表示和后续修复中处理这一点，例如在选择时可屏蔽（mask）掉某些已选择或不合理的动作。

模型结构与算法

DRL 代理采用神经网络实现策略函数，将环境状态映射到动作选择。模型结构可能包括：

特征融合网络：首先将每个候选任务的特征输入若干层神经网络（例如多层感知器或者图神经网络）提取高维表示。可使用注意力机制汇总全局信息，捕捉任务之间关系
papers.ssrn.com
papers.ssrn.com
。例如，对于车辆路径问题，可融合任务距离、时间窗重叠等信息，识别任务簇特征。

策略网络 (Policy Network)：在逐步选择架构下，策略网络根据当前状态输出每个剩余任务被选为下一个移除的概率。可采用Softmax输出一个概率分布，或直接输出评分选取最大值。如果采用一次性选择模型，则策略网络可输出一个任务集合的指示向量。

价值网络 (Value Network)：如果使用Actor-Critic算法训练，可引入价值网络估计当前状态的价值，用于降低策略训练的方差。价值网络可与策略网络共享部分层（即采用Actor-Critic框架）。

强化学习算法：训练时可以选用策略梯度方法（如 REINFORCE, PPO 等）或 Q-learning 方法。基于本问题连续决策特点，常用的是Actor-Critic策略梯度。训练算法通过反复模拟移除-重插过程更新网络参数，使得策略网络倾向于选择能改进解的动作序列。

训练过程

DRL 模块的训练在本框架中可独立进行，通常通过单独的训练脚本或实验 (例如实验 E5) 完成：

环境交互：每次训练迭代从数据生成模块获取一个随机问题实例，构造初始解。然后在此解上让代理反复执行移除k个任务（按照之前描述的决策方式），每次完整的移除-修复过程结束后计算奖励。

经验积累：收集多轮次的状态、动作、奖励序列作为经验样本。为提高样本效率，可并行生成多个实例的经验或者使用经验回放缓存。

参数更新：根据选定的RL算法，对策略网络（以及价值网络）执行梯度更新。目标是提高在给定状态下选择带来高奖励动作的概率。比如采用策略梯度时，根据累计奖励提升所选动作概率；采用价值网络时则基于优势函数调整策略。

重复训练：循环执行环境模拟和参数更新，直至达到预设的训练回合数或性能收敛。训练过程中会定期评估代理在验证集上的表现，避免过拟合或策略崩溃。

模型保存：训练结束后，将学习到的模型参数保存到 models/ 目录下（例如生成 drl_model.pth 文件）。后续实验运行时可加载此模型用于推理。

训练过程会输出日志和曲线，例如每回合的平均奖励、策略收敛趋势等，方便观察DRL策略的学习进展。在 Results.md 会提到这些输出形式。

推理与集成

训练完成后，DRL 模块作为一个推理服务嵌入HEA算法运行：

模型加载：在实验开始时，程序会根据配置加载预先训练好的DRL模型参数。若模型文件不存在或路径错误，将提示用户训练模型或检查配置。

决策调用：当HEA核心需要通过DRL策略决定移除任务时，会调用DRL模块提供的接口函数，例如 agent.select_removal(solution, k)。模块接收当前解的状态表示，利用内部神经网络计算输出，返回要移除的任务列表。

时间性能：一次DRL决策涉及神经网络运算。相比简单启发式略有开销，但通过批量计算和GPU加速（如果可用）可保证推理在可接受时间内完成。本模块对每次推理调用进行计时记录（详见 Timing.md 中DRL推理时间），以便分析DRL策略对整体效率的影响。

可插拔接口：若需停用DRL策略，只需在配置中将移除策略切换为其它，即不会调用该模块。DRL模块的接口设计使其对HEA核心是透明的——核心并不关心具体策略实现，只看返回结果。因此切换不同策略不会影响HEA主流程。

使用方法与配置

训练阶段：可通过运行专门的训练脚本开始DRL策略训练，例如执行 python train_drl.py --config configs/train_DRL.yaml（具体命令和配置参见 Experiments E5）。训练配置文件中包含诸如训练代数、学习率、批量大小、探索策略等参数，可根据需要调整。

推理阶段：在进行正式求解实验时（E1–E4），确保在对应的配置文件中将 removal_strategy 设置为 "DRL"，并提供 model_path 字段指向正确的模型文件。程序会在开始时自动加载模型，无需人工干预。

模型更新：如需使用新的训练模型，只需更新配置的模型路径或替换模型文件。框架支持随时替换DRL策略，实现例如对比不同训练版本模型的效果。

通过以上机制，DRL模块无缝地融入了HEA框架，提供了强大的数据驱动决策能力。其训练可以离线完成，而推理阶段与其他启发式策略并行替换，具备很高的灵活性和实用性。